# Final Report : CTests for Apache Skywalking


# Problem

1. Exercise software code under different configuration values and detect failure inducing configuration changes using [CTests](https://www.usenix.org/system/files/osdi20-sun.pdf) in the [Apache Skywalking](https://github.com/apache/skywalking) project.
2. Use [Git Bisect](https://git-scm.com/docs/git-bisect) in context of flaky tests to find the commit that fixed the flaky test in an automated way.

# Summary
The Apache Skywalking project follows a hierarchical configuration structure of the form -
```yaml
Module
  Provider
    Property (this is of type java.util.Property)
      Sub-Property (this is of type java.util.Property)
```
The main goal of this project was to enable CTests for hierarchical configurations in Skywalking. 
This involved analysis and design to parse nested config structures, generating valid config values, identifying CTests and finally injecting different config values to detect failure inducing configs.

### Results
1. Raised [PR-12](https://github.com/xlab-uiuc/openctest/pull/12) to [xlab-uiuc/openctest](https://github.com/xlab-uiuc/openctest) to enable CTests for Skywalking
   - This PR includes 3 new java modules - Identify Params, Generate CTest and Run CTest to enable CTests for the nested config structure in Skywalking
   - This also provides CTest support to 6 Skywalking modules 
   - The same framework can be extended to the other 81 Skywalking modules
2. Merged [PR-635](https://github.com/TestingResearchIllinois/idoft/pull/635) to [IDoFT](https://github.com/TestingResearchIllinois/idoft) which adds a script to automatically detect the commit that fixed the flaky test using Git Bisect

# Total Effort
I spent **148 hours** on this project throughout the Fall 2022 semester. Out of which 20 hours were spent on Git Bisect - analysis, implementation of the script and testing, and the remaining 128 hours on CTests.

# Longer Description

### Cumulative Progress

1. Added a new script [core/add_project_skywalking.sh](https://github.com/xlab-uiuc/openctest/pull/12/files#diff-e8856b0b885811875e49c175b39e0a1a268c4792073931d78e73eda237220240) to automatically set up the skywalking project for a new release version. 
I worked on the latest release [9.3.0](https://github.com/apache/skywalking/releases/tag/v9.3.0).
2. Completed CTests for 6 modules -
```java
1. oap-server/server-starter
2. oap-server/server-configuration/configuration-apollo
3. oap-server/server-configuration/configuration-consul
4. oap-server/server-configuration/configuration-zookeeper
5. oap-server/analyzer/agent-analyzer
6. oap-server/server-configuration/configuration-nacos
```
3. Four of the above modules run test cases on a docker runtime. Steps to set up the docker environment are added to the [`core/setup_ubuntu_skywalking.sh`](https://github.com/xlab-uiuc/openctest/pull/12/files#diff-18a3b7149ee361a92bf45434d6bf529b242d5314dad1e3c26e1cfa35df791c3f) script.
4. Code is instrumented to identify params and inject config values for a hierarchical config structure. This is achieved by the [logging.patch](https://github.com/xlab-uiuc/openctest/pull/12/files#diff-a3b1b8d46c85d8d8000d0816c3fcd1ef7964c89dfa257f44539d5d13faeec373) and [interception.patch](https://github.com/xlab-uiuc/openctest/pull/12/files#diff-7eaed231572ff02d0e57ef0735ebea33e460683b680aed409d1e243d7593e566). A custom deep merge function was written to override the base yaml file with the injected nested yaml configs.
5. Implemented java code [`core/identify_param/skywalking`](https://github.com/xlab-uiuc/openctest/pull/12/files#diff-4432c2ad2622a714f6c1061a489e1ae7cac1c21237975a2a83f7dcaf3d0f32a4) which is used to identify the configs used by each test case in skywalking.
6. Updated python code [`core/generate_value/value_generation.py`](https://github.com/xlab-uiuc/openctest/pull/12/files#diff-a4e450cd46dd19bca660c274a988509bccdb67957c032f13d22ac42153a08124) to generate valid config values for each param. This python program takes a flat key value input. So, code to convert a hierarchical config structure to a flat structure is also present in the [`core/identify_param/skywalking`](https://github.com/xlab-uiuc/openctest/pull/12/files#diff-4432c2ad2622a714f6c1061a489e1ae7cac1c21237975a2a83f7dcaf3d0f32a4) jar.
7. Implemented java code [`core/generate_ctest/skywalking`](https://github.com/xlab-uiuc/openctest/pull/12/files#diff-53b0abeff9b93a727d5e40206a34aba749504ba153aaa24b18d5e53e598dcf42) which is used to run the test cases against the corresponding valid config values and generate the CTest json [`core/generate_ctest/ctest_mapping/ctests-skywalking.json`](https://github.com/xlab-uiuc/openctest/pull/12/files#diff-28086230e14901ee051def39af5e081a063e54e1f9bb65b867cf7041e9319bd3).
8. Implemented java code [`core/run_ctest/skywalking`](https://github.com/xlab-uiuc/openctest/pull/12/files#diff-dc1e1256a4bfddad3406a5aebbba34783413993457e8d9a69b8f0732a350c2bb) which is used to run a test case against injected config values (config injection happens with a yml file as an input).
9. Executed test cases against good and bad config values and checked for test failures for bad value injections.

### Progress since the last report
1. Added a new script [core/add_project_skywalking.sh](https://github.com/xlab-uiuc/openctest/pull/12/files#diff-e8856b0b885811875e49c175b39e0a1a268c4792073931d78e73eda237220240) to automatically set up the skywalking project for a new release version.
2. Used the script to set up the latest Skywalking release [9.3.0](https://github.com/apache/skywalking/releases/tag/v9.3.0) and re-tested all the CTest modules.
3. Detailed documentation about running each of the above modules was added in the readMe files.
4. Added description for each property in the [core/default_configs/skywalking-default.tsv](https://github.com/xlab-uiuc/openctest/pull/12/files#diff-0f7e702479483467ebdf8f354396b001d7ba1f2ad239b22523917ba04b05da17) file.
5. Generated the patch files to instrument the code and added the [logging.patch](https://github.com/xlab-uiuc/openctest/pull/12/files#diff-a3b1b8d46c85d8d8000d0816c3fcd1ef7964c89dfa257f44539d5d13faeec373) and [interception.patch](https://github.com/xlab-uiuc/openctest/pull/12/files#diff-7eaed231572ff02d0e57ef0735ebea33e460683b680aed409d1e243d7593e566) files to the openctest PR.

# Questions and Answers

### What are the pain points in working on the configuration project?

I had 4 main pain points while working on CTests for Apache Skywalking 
1. Most of the modules needed a **docker runtime** to run the test cases. Docker had to be installed and setup to run these CTests.
2. Some modules in Skywalking were **not generating logs**. Without the logs I could not identify the test caseâ€™s params. The `log4j2.xml` file had to be added to the modules to generate logs. 
3. In the skywalking configuration hierarchy, levels 3 and 4 are of types **java.util.Property**. Instrumenting this code is not directly possible. So, I had to write wrapper methods over these library methods and replace all occurrences in code.
4. I was not able to clone and **build** the skywalking project as it had issues loading the dependencies. So, I had to build the project from the [source code release](https://github.com/apache/skywalking/blob/master/docs/en/guides/How-to-build.md#building-from-apache-source-code-release). 

### How much time does the project take in total and what is the most time-consuming part?
I spent 128 hours on the CTest project throughout the Fall 2022 semester. The most time-consuming parts were -
1. Analysis of the hierarchical configuration structure used in the Skywalking project and designing a way to identify params and inject values for such configuration structures. 
2. The identify-param module, generate-ctest module and run-ctest module had to be rewritten (I used java) to support the hierarchical configuration structure. Implementation of these 3 java modules considering nested structures took the maximum time. 

### What part of the original Ctest model (in the [paper](https://www.usenix.org/conference/osdi20/presentation/sun)) works well and what works poorly?

`Find and modify the GET/SET API` step in CTest had some problems for the Skywalking project because the instrumentation of the GET and SET API was not directly possible. 
The last 2 levels of hierarchy was of the form java.util.Property which needed a custom wrapper to instrument the code. This also meant that all calls to this function had to be replaced with the new wrapper functions.

The `generate value` step uses a tab separated file as an input but does not work with a hierarchy config structure. So, custom code had to be written to flatten the hierarchy structure to generate valid values.

All the remaining modules `identify parameters`, `generate ctest` and `run ctest` needed custom code to support the config structure. 
But, logically followed the same steps as presented in the original CTest model, and they worked well. I was able to generate CTests and test them against good and bad config values even for nested structures.

### How good or bad is the Ctest prototype?

The CTest prototype is very effective at detecting and preventing failure-inducing configs.
It helps us identify configurations that can cause code failures and detect bugs.
Avoiding misconfigurations by testing all configuration changes in the context of code will also provide confidence in the production deployment of these configs.


#### 1. How often/efficiently can Ctest pinpoint the BAD value of the parameters?

CTest are very effective in pinpointing the bad config values. For the Skywalking project, all the expected failure-inducing configurations resulted in test case failures.

From my experience running CTests, detecting the failure-inducing config works very efficiently for configs that map to a Java default type like enum or to a Boolean type.

However, for other datatypes such as String or Integer, unless there is an explicit validation in code, pinpointing a bad value can be difficult. 
We often rely on out-of-bound values, values from different datatypes, or out-of-range values.

Also, there are cases where valid config values can still cause PROD failures. For example, if a change in config is causing the method execution time to increase or increase in the memory utilization, or perhaps increase in the API response time. Unless an explicit test case covers these scenarios, bad config values cannot be detected in such cases. 

#### 2. If you are the boss of one famous project (spark, nifi, etc...) Are you willing to spend some time supporting the configuration testing for the project? Why or Why not?

Yes, CTest is a very efficient and necessary tool for large applications which have a large set of frequently updated configs.

If CTests could be automated for every new test case added and every new config added then CTests could be added to the CI pipeline running every night which will help apps that have frequent config additions. 

Every PROD config deployment can first be tested using CTests to make sure that we don't find any unexpected behavior in PROD.
This also means that failure-inducing configs can be detected early on which can avoid prod failures. With apps like nifi or spark which are used by multiple systems, a prod failure has a massive impact that can be detected and corrected at an early stage.

Also, CTest setup, in general, will need addition of a few lines of code and for projects with centralized config handling, this will have minimal impact. 
For the benefits that CTests offer, I will be willing to spend time enabling CTests. 

#### 3. Is there any disadvantages of applying the configuration testing to more projects?
A few disadvantages can be -
1. Heavy Dependence on Test Cases - For CTests to work efficiently, we need good test coverage, not only in terms of the quantity of test cases but also in terms of the quality of tests. For every new project, assessing the quality of test cases before implementing CTests is a difficult task.
2. Maintenance - Unless the instrumentation code is integrated with the main codebase, maintaining it will be necessary for every update to the trunk. That is, the instrumentation code should be updated to support every new release if we plan to always work on the latest codebase.
3. Additional Efforts - The addition of new ways to read configs in an application means that the CTest code (logging and interception) should also be updated. For example - in Skywalking different modules read configs in different ways which means different instrumentation and value injection code.
4. Time-Consuming - Detecting CTests involves running each test runs against a set of configs which is a very time-consuming process and can take hours to execute. Optimizing this and running the same test against a set of config values in parallel would help us reduce the execution time. 

### How does Ctest fit into projects that do not have key-value format configurations? How to fit those interfaces into Ctest? How to track those configuration parameters?

CTest works well for projects without the standard key-value config format as well.
However, enabling CTests for such projects involves complex steps during the initial setup phase.

Tracking the config parameters involves the generation of a TestCase-to-Config map where the configs used by each test will have a nested structure. For example :

```json
"org.apache.skywalking.oap.server.starter.config.ApplicationConfigLoaderTestCase#testLoadConfig": {
      "storage": {
        "mysql": {
          "metadataQueryMaxSize": "5000",
          "properties": {
            "jdbcUrl": "jdbc:mysql://localhost:3306/software?rewriteBatchedStatements"
          }
        }
      }
    }...
```

Fitting these interfaces to CTests mainly involves the following steps -  
1. **Instrumentation** - The instrumentation step involves adding logs to SET and GET methods at every level of config. This is to capture reads and writes at every level of the nested config.
2. **Identifying Params** - This step uses the instrumented code to generate the config structure as seen above.
3. **Generate Values** - No major updates to this step. The only difference is that it will need flattening of the nested structure to use the existing value generation algorithm.
4. **Generate CTest** - Running all tests against valid config values will involve generating individual config files (in a yaml format) for each valid config to maintain the hierarchy. These files will be used in the value injection step to generate the CTest json file.  
5. **Run CTest** - Will need a yaml file as an input which overrides the base yaml file at any level of nesting. 
